{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Age                2000 non-null   int64  \n",
      " 1   Gender             2000 non-null   object \n",
      " 2   Health_Status      2000 non-null   object \n",
      " 3   Marital_Status     2000 non-null   object \n",
      " 4   Current_Insurance  1664 non-null   object \n",
      " 5   Previous_Policies  2000 non-null   int64  \n",
      " 6   Claim_History      2000 non-null   object \n",
      " 7   Annual_Income      1950 non-null   float64\n",
      " 8   Premium            2000 non-null   int64  \n",
      " 9   Sum_Assured        2000 non-null   int64  \n",
      " 10  Family_Details     1613 non-null   object \n",
      "dtypes: float64(1), int64(4), object(6)\n",
      "memory usage: 172.0+ KB\n",
      "None\n",
      "\n",
      "Processed columns: ['Age', 'Gender', 'Health_Status', 'Marital_Status', 'Claim_History', 'Annual_Income', 'Premium', 'Sum_Assured', 'Income_to_Age', 'Premium_to_Income']\n",
      "Processed dtypes: Age                  float64\n",
      "Gender                 int64\n",
      "Health_Status          int64\n",
      "Marital_Status         int64\n",
      "Claim_History          int64\n",
      "Annual_Income        float64\n",
      "Premium                int64\n",
      "Sum_Assured          float64\n",
      "Income_to_Age        float64\n",
      "Premium_to_Income    float64\n",
      "dtype: object\n",
      "Features shape: (2000, 9)\n",
      "Target shape: (2000,)\n",
      "Model input size: 9\n",
      "Epoch [10/1000], Train Loss: 8406744.2200, Val Loss: 8350378.0385\n",
      "Epoch [20/1000], Train Loss: 8273388.2100, Val Loss: 8219801.4615\n",
      "Epoch [30/1000], Train Loss: 8089352.7600, Val Loss: 8045955.6538\n",
      "Epoch [40/1000], Train Loss: 7858952.2700, Val Loss: 7801667.4615\n",
      "Epoch [50/1000], Train Loss: 7600292.0800, Val Loss: 7563924.8846\n",
      "Epoch [60/1000], Train Loss: 7305853.2900, Val Loss: 7302512.2308\n",
      "Epoch [70/1000], Train Loss: 6977505.2400, Val Loss: 6989086.4231\n",
      "Epoch [80/1000], Train Loss: 6630528.3100, Val Loss: 6658934.9615\n",
      "Epoch [90/1000], Train Loss: 6241479.0000, Val Loss: 6289494.0769\n",
      "Epoch [100/1000], Train Loss: 5853474.3600, Val Loss: 5992926.3077\n",
      "Epoch [110/1000], Train Loss: 5441820.7100, Val Loss: 5547006.6923\n",
      "Epoch [120/1000], Train Loss: 5036370.0500, Val Loss: 5120666.0769\n",
      "Epoch [130/1000], Train Loss: 4546903.6900, Val Loss: 4707962.3269\n",
      "Epoch [140/1000], Train Loss: 4180890.3750, Val Loss: 4354698.0962\n",
      "Epoch [150/1000], Train Loss: 3761080.8200, Val Loss: 3904967.1154\n",
      "Epoch [160/1000], Train Loss: 3385515.8425, Val Loss: 3590537.6731\n",
      "Epoch [170/1000], Train Loss: 2946897.7300, Val Loss: 3166752.4231\n",
      "Epoch [180/1000], Train Loss: 2608724.2050, Val Loss: 2855590.2308\n",
      "Epoch [190/1000], Train Loss: 2266268.4325, Val Loss: 2472798.8462\n",
      "Epoch [200/1000], Train Loss: 1993826.3162, Val Loss: 2056390.9712\n",
      "Epoch [210/1000], Train Loss: 1686740.4225, Val Loss: 1985497.2885\n",
      "Epoch [220/1000], Train Loss: 1394913.9013, Val Loss: 1540563.7885\n",
      "Epoch [230/1000], Train Loss: 1211710.2225, Val Loss: 1414364.0192\n",
      "Epoch [240/1000], Train Loss: 1028024.5675, Val Loss: 1133309.9567\n",
      "Epoch [250/1000], Train Loss: 856383.5487, Val Loss: 964227.2404\n",
      "Epoch [260/1000], Train Loss: 739944.7256, Val Loss: 752473.7692\n",
      "Epoch [270/1000], Train Loss: 632261.1466, Val Loss: 507783.0264\n",
      "Epoch [280/1000], Train Loss: 517774.1869, Val Loss: 529393.8918\n",
      "Epoch [290/1000], Train Loss: 467180.5098, Val Loss: 432190.3077\n",
      "Epoch [300/1000], Train Loss: 429809.0909, Val Loss: 342057.3798\n",
      "Epoch [310/1000], Train Loss: 361203.1622, Val Loss: 292247.5397\n",
      "Epoch [320/1000], Train Loss: 365363.4827, Val Loss: 334393.0565\n",
      "Epoch [330/1000], Train Loss: 325687.9525, Val Loss: 149833.3882\n",
      "Epoch [340/1000], Train Loss: 318004.9950, Val Loss: 114603.2948\n",
      "Epoch [350/1000], Train Loss: 325260.7716, Val Loss: 110322.7978\n",
      "Epoch [360/1000], Train Loss: 317887.3094, Val Loss: 78022.9130\n",
      "Epoch [370/1000], Train Loss: 302825.8569, Val Loss: 103932.3849\n",
      "Epoch [380/1000], Train Loss: 231403.7205, Val Loss: 48630.1127\n",
      "Early stopping at epoch 387\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # Add this import\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def verify_columns(df):\n",
    "    \"\"\"Verify and print available columns\"\"\"\n",
    "    print(\"\\nAvailable columns in dataset:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"- {col}\")\n",
    "    return df.columns.tolist()\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess data with proper type conversion\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Handle categorical features\n",
    "    cat_cols = ['Gender', 'Health_Status', 'Marital_Status', 'Claim_History']\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in cat_cols:\n",
    "        label_encoders[col] = LabelEncoder()\n",
    "        df[col] = label_encoders[col].fit_transform(df[col].astype(str))\n",
    "\n",
    "    # 2. Handle numeric columns with proper error handling\n",
    "    numeric_cols = ['Age', 'Annual_Income', 'Sum_Assured', 'Premium']\n",
    "    for col in numeric_cols:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Fill numeric nulls with median\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {col}: {str(e)}\")\n",
    "\n",
    "    # 3. Drop problematic columns\n",
    "    cols_to_drop = [\n",
    "        'Current_Insurance',\n",
    "        'Previous_Policies',\n",
    "        'Family_Details'  # Will handle family details separately if needed\n",
    "    ]\n",
    "    df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # 4. Feature engineering (only after numeric conversion)\n",
    "    df['Income_to_Age'] = df['Annual_Income'] / df['Age']\n",
    "    df['Premium_to_Income'] = df['Premium'] / df['Annual_Income']\n",
    "\n",
    "    # 5. Scale numeric features\n",
    "    num_cols_to_scale = [\n",
    "        'Age',\n",
    "        'Annual_Income',\n",
    "        'Sum_Assured',\n",
    "        'Income_to_Age',\n",
    "        'Premium_to_Income'\n",
    "    ]\n",
    "\n",
    "    # Replace infinities before scaling\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df[num_cols_to_scale] = df[num_cols_to_scale].fillna(\n",
    "        df[num_cols_to_scale].median())\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    df[num_cols_to_scale] = scaler.fit_transform(df[num_cols_to_scale])\n",
    "\n",
    "    return df, scaler, label_encoders\n",
    "\n",
    "\n",
    "class InsuranceDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        # Ensure all inputs are float32\n",
    "        self.features = torch.FloatTensor(features.astype(np.float32))\n",
    "        self.targets = torch.FloatTensor(\n",
    "            targets.astype(np.float32)).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "class ImprovedInsuranceModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Print input size for debugging\n",
    "        print(f\"Model input size: {input_size}\")\n",
    "\n",
    "        # Layer architecture\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = F.relu(x1)\n",
    "        x1 = self.dropout(x1)\n",
    "\n",
    "        # Second layer with residual\n",
    "        x2 = self.fc2(x1)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = F.relu(x2)\n",
    "        x2 = self.dropout(x2)\n",
    "        x2 = x2 + x1[:, :32]  # Residual connection\n",
    "\n",
    "        # Third layer\n",
    "        x3 = self.fc3(x2)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = F.relu(x3)\n",
    "        x3 = self.dropout(x3)\n",
    "\n",
    "        # Output layer\n",
    "        out = self.fc4(x3)\n",
    "        return F.relu(out)  # Ensure positive output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, scaler, encoders, epochs=1000):\n",
    "    model = model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'scaler': scaler,\n",
    "                'encoders': encoders,\n",
    "                'input_size': model.fc1.in_features\n",
    "            }, 'insurance_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss /\n",
    "                  len(train_loader):.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "def calculate_sum_assured(age, annual_income):\n",
    "    \"\"\"More conservative sum assured calculation\"\"\"\n",
    "    if age < 30:\n",
    "        multiplier = 15\n",
    "    elif age < 40:\n",
    "        multiplier = 12\n",
    "    elif age < 50:\n",
    "        multiplier = 8\n",
    "    else:\n",
    "        multiplier = 5\n",
    "    return min(annual_income * multiplier, 30000000)  # Cap at 3 Cr\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load data with proper encoding\n",
    "        df = pd.read_csv(\n",
    "            'K:/Insurance-Prediction/insurance_data.csv', encoding='utf-8')\n",
    "\n",
    "        # Print data info for debugging\n",
    "        print(\"\\nDataset Info:\")\n",
    "        print(df.info())\n",
    "\n",
    "        processed_df, scaler, encoders = preprocess_data(df)\n",
    "\n",
    "        # Verify processed data\n",
    "        print(\"\\nProcessed columns:\", processed_df.columns.tolist())\n",
    "        print(\"Processed dtypes:\", processed_df.dtypes)\n",
    "\n",
    "        # Split features and target\n",
    "        X = processed_df.drop('Premium', axis=1).values  # Convert to numpy\n",
    "        y = processed_df['Premium'].values  # Convert to numpy\n",
    "\n",
    "        print(f\"Features shape: {X.shape}\")\n",
    "        print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "        # Train/val split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = InsuranceDataset(X_train, y_train)\n",
    "        val_dataset = InsuranceDataset(X_val, y_val)\n",
    "\n",
    "        # Create loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "        # Initialize model\n",
    "        model = ImprovedInsuranceModel(input_size=X.shape[1])\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=0.001, weight_decay=0.01\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        train_model(model, train_loader, val_loader,\n",
    "                    criterion, optimizer, device, scaler, encoders)\n",
    "\n",
    "        # Save model components\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'scaler': scaler,\n",
    "            'encoders': encoders\n",
    "        }, 'insurance_model.pth')\n",
    "\n",
    "        return model, scaler, encoders\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main(): {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, scaler, encoders = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Recommended Insurance Plan: ULIP\n",
      "    Premium: ₹9,645.86/year\n",
      "    Sum Assured: ₹12,000,000.00\n",
      "    Term: 25 years\n",
      "    Conversion Probability: 70.0%\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "class ImprovedInsuranceModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Single output for premium\n",
    "        self.premium_head = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        premium = self.premium_head(features)\n",
    "        return F.relu(premium)  # Ensure positive premium\n",
    "\n",
    "def prepare_features(age, gender, health_status, marital_status, \n",
    "                    annual_income, claim_history, scaler, encoders):\n",
    "    \"\"\"Prepare input features for prediction\"\"\"\n",
    "    \n",
    "    # Create input dataframe\n",
    "    input_data = pd.DataFrame({\n",
    "        'Age': [age],\n",
    "        'Gender': [gender],\n",
    "        'Health_Status': [health_status],\n",
    "        'Marital_Status': [marital_status],\n",
    "        'Annual_Income': [annual_income],\n",
    "        'Claim_History': [claim_history]\n",
    "    })\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_maps = {\n",
    "        'Gender': {'Male': 0, 'Female': 1},\n",
    "        'Health_Status': {'Healthy': 0, 'Minor Issues': 1, 'Chronic': 2},\n",
    "        'Marital_Status': {'Single': 0, 'Married': 1, 'Divorced': 2, 'Widowed': 3},\n",
    "        'Claim_History': {'None': 0, 'Minor': 1, 'Major': 2}\n",
    "    }\n",
    "    \n",
    "    # Apply categorical mappings\n",
    "    for col, mapping in categorical_maps.items():\n",
    "        input_data[col] = input_data[col].map(mapping)\n",
    "    \n",
    "    # Calculate Sum_Assured before feature engineering\n",
    "    input_data['Sum_Assured'] = calculate_initial_sum_assured(age, annual_income)\n",
    "    \n",
    "    # Add engineered features\n",
    "    input_data['Income_to_Age'] = input_data['Annual_Income'] / input_data['Age']\n",
    "    input_data['Premium_to_Income'] = 0  # placeholder for new predictions\n",
    "    \n",
    "    # Ensure columns match training data order\n",
    "    required_columns = [\n",
    "        'Age', 'Gender', 'Health_Status', 'Marital_Status', \n",
    "        'Claim_History', 'Annual_Income', 'Sum_Assured',\n",
    "        'Income_to_Age', 'Premium_to_Income'\n",
    "    ]\n",
    "    input_data = input_data[required_columns]\n",
    "    \n",
    "    # Scale numeric features\n",
    "    numeric_cols = ['Age', 'Annual_Income', 'Sum_Assured', 'Income_to_Age', 'Premium_to_Income']\n",
    "    input_data[numeric_cols] = scaler.transform(input_data[numeric_cols])\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.FloatTensor(input_data.values).to(device)\n",
    "    \n",
    "    return input_tensor\n",
    "\n",
    "def calculate_initial_sum_assured(age, annual_income):\n",
    "    \"\"\"Calculate initial sum assured for feature preparation\"\"\"\n",
    "    if age < 30:\n",
    "        multiplier = 15\n",
    "    elif age < 40:\n",
    "        multiplier = 12\n",
    "    elif age < 50:\n",
    "        multiplier = 8\n",
    "    else:\n",
    "        multiplier = 5\n",
    "    return min(annual_income * multiplier, 30000000)  # Cap at 3 Cr\n",
    "\n",
    "def predict_insurance_details(\n",
    "    age, gender, health_status, marital_status,\n",
    "    annual_income, claim_history, model, scaler, device\n",
    "):\n",
    "    try:\n",
    "        # Prepare input features\n",
    "        input_tensor = prepare_features(\n",
    "            age, gender, health_status, marital_status,\n",
    "            annual_income, claim_history, scaler, encoders\n",
    "        )\n",
    "        \n",
    "        # Get model prediction (just premium)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            premium = model(input_tensor)\n",
    "            \n",
    "        # Calculate recommended policy type based on demographics\n",
    "        policy_types = ['Term Plan', 'Endowment Plan', 'ULIP', \n",
    "                       'Retirement Plan', 'Child Plan']\n",
    "        \n",
    "        # Simple policy recommendation logic\n",
    "        if age < 35 and marital_status == 'Single':\n",
    "            recommended_policy = 'Term Plan'\n",
    "        elif age > 45:\n",
    "            recommended_policy = 'Retirement Plan'\n",
    "        elif marital_status == 'Married' and age < 40:\n",
    "            recommended_policy = 'ULIP'\n",
    "        elif health_status != 'Healthy':\n",
    "            recommended_policy = 'Endowment Plan'\n",
    "        else:\n",
    "            recommended_policy = 'Term Plan'\n",
    "            \n",
    "        # Calculate sum assured\n",
    "        sum_assured = calculate_initial_sum_assured(age, annual_income)\n",
    "        \n",
    "        # Calculate recommended term\n",
    "        if age < 30:\n",
    "            recommended_term = 30\n",
    "        elif age < 40:\n",
    "            recommended_term = 25\n",
    "        elif age < 50:\n",
    "            recommended_term = 20\n",
    "        else:\n",
    "            recommended_term = 15\n",
    "            \n",
    "        # Estimate conversion probability\n",
    "        conversion_prob = 0.7 if health_status == 'Healthy' else 0.4\n",
    "        \n",
    "        return {\n",
    "            'recommended_policy': recommended_policy,\n",
    "            'premium': premium.item(),\n",
    "            'sum_assured': sum_assured,\n",
    "            'term_years': recommended_term,\n",
    "            'conversion_probability': conversion_prob\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test the prediction\n",
    "sample_prediction = predict_insurance_details(\n",
    "    age=35,\n",
    "    gender='Male',\n",
    "    health_status='Healthy',\n",
    "    marital_status='Married', \n",
    "    annual_income=1000000,\n",
    "    claim_history='None',\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "if sample_prediction:\n",
    "    print(f\"\"\"\n",
    "    Recommended Insurance Plan: {sample_prediction['recommended_policy']}\n",
    "    Premium: ₹{sample_prediction['premium']:,.2f}/year\n",
    "    Sum Assured: ₹{sample_prediction['sum_assured']:,.2f}\n",
    "    Term: {sample_prediction['term_years']} years\n",
    "    Conversion Probability: {sample_prediction['conversion_probability']:.1%}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance_prediction_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
